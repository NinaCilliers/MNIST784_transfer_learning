{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digit classification via transfer learning\n",
    "\n",
    "... to decode the MNIST 784 dataset. This dataset contains a subset of 70,000 size-normalized and centered hand written digits. Previously a convolutional neural network was built with 99.6% accuracy.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, MobileNet\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\corne\\\\OneDrive\\\\Documents\\\\DS_Portfolio\\\\MNIST784_transfer_learning'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\corne\\\\OneDrive\\\\Documents\\\\DS_Portfolio\\\\MNIST784_transfer_learning')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fully connected training iterations\n",
    "fc_iter = 1\n",
    "\n",
    "#fine tuning traiing iterations\n",
    "ft_iter = 1\n",
    "\n",
    "iters = (fc_iter, ft_iter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading MNIST_784 dataset from OpenML\n",
    "mnist = fetch_openml('mnist_784', as_frame = False, parser='auto') \n",
    "X,y = mnist.data.reshape(-1,28,28,1), mnist.target.reshape(-1,1)\n",
    "y = y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X range before rescaling:\n",
      "0 255\n",
      "X range after rescaling\n",
      "-1.0 1.0\n",
      "X mean: -0.74\n",
      " X std: 0.62\n"
     ]
    }
   ],
   "source": [
    "#rescaling X\n",
    "print('X range before rescaling:')\n",
    "print(round(X[0].min(),2), round(X[0].max(),2))\n",
    "\n",
    "X = X/127.5-1\n",
    "print('X range after rescaling')\n",
    "print(round(X[0].min(),2), round(X[0].max(),2))\n",
    "print(f'X mean: {round(X.mean(),2)}')\n",
    "print(f' X std: {round(X.std(),2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to look at a digit\n",
    "def show_num(input_pic):\n",
    "  plt.imshow(input_pic,cmap='binary')\n",
    "  plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape before:\n",
      "(70000, 28, 28, 1)\n",
      "Image shape after:\n",
      "(70000, 28, 28, 3)\n",
      "\n",
      "Checking Image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#repeating gray scale image in 3 channel format for compatability with three channel format \n",
    "#of the pre-trained network\n",
    "\n",
    "print('Image shape before:')\n",
    "print(X.shape)  \n",
    "X_rgb = np.repeat(X,3,axis=-1)\n",
    "print('Image shape after:')\n",
    "print(X_rgb.shape)\n",
    "\n",
    "print('\\nChecking Image:')\n",
    "#checking repeat image \n",
    "for n in range(3):\n",
    "    ax = plt.subplot(1,3,n+1)\n",
    "    show_num(X_rgb[0][:,:,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making test and train sets\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = X_rgb[:60000], y[:60000],X_rgb[60000:65000],y[60000:65000],X_rgb[65000:],y[65000:]\n",
    "\n",
    "#temporary \n",
    "X_train = X_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "X_valid = X_valid[:1000]\n",
    "y_valid = y_valid[:1000]\n",
    "\n",
    "\n",
    "train_data = (X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training transfer learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating transfer learning model\n",
    "\n",
    "def build(t_model, input_size):\n",
    "    #creates transfer model\n",
    "    transfer_model = t_model(include_top=False, input_shape=(input_size,input_size,3),weights='imagenet')\n",
    "    transfer_model.trainable = False #freezes model \n",
    "    #note: setting trainable to false also sets training to false for BN layers in tf version 2.0 and above\n",
    "\n",
    "    #creates model with fully connected layers\n",
    "    print('Building model...')\n",
    "    inputs = tf.keras.Input(shape=(28,28,3), name='Input')\n",
    "    x = tf.keras.layers.Resizing(input_size, input_size)(inputs)\n",
    "    x = transfer_model(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(units=128, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(units=10,activation='linear',kernel_initializer='he_normal')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    \n",
    "    #compile model \n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            optimizer = tf.keras.optimizers.Adam(0.001),\n",
    "            metrics = ['accuracy'])\n",
    "    \n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_FC(model, train_data, iters):\n",
    "    #training top layers\n",
    "    print('Training top layers...')\n",
    "    history = model.fit(train_data[0], train_data[1], validation_data=(train_data[2:]), epochs = iters[0])\n",
    "\n",
    "    #saving weights\n",
    "    weight_name = 'weights_'+str(model.layers[2].name)+'_fc.h5'\n",
    "    model.save_weights(weight_name)\n",
    "    return(model, history)\n",
    "    #save_weights = ModelCheckpoint(weight_file_name, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, train_data, iters):\n",
    "    print('Fine tuning all layers...')\n",
    "    \n",
    "    #unfreezing transfer model\n",
    "    model.layers[2].trainable = True \n",
    "    #print(str(model.layers[2].name),str(model.layers[2].trainable))\n",
    "\n",
    "    #Checking that BN layers are in inference mode and all layers are trainable\n",
    "    for layer in model.layers[2].layers: \n",
    "        if (isinstance(layer, tf.keras.layers.BatchNormalization)):\n",
    "            layer.trainable = False\n",
    "        else: assert layer.trainable == True\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer = tf.keras.optimizers.Adam(1e-6),\n",
    "        metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data[0], train_data[1], validation_data = (train_data[2:]), epochs = iters[1])\n",
    "\n",
    "    #saving weights\n",
    "    weight_name = 'weights_'+str(model.layers[2].name)+'_ft.h5'\n",
    "    model.save_weights(weight_name)\n",
    "\n",
    "    return(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(t_model, input_size, train_data, iters):\n",
    "    model = build(t_model, input_size)\n",
    "\n",
    "    model, history = train_FC(model, train_data, iters)\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    model, history = fine_tune(model, train_data, iters)\n",
    "    acc += history.history['accuracy']\n",
    "    val_acc += history.history['val_accuracy']\n",
    "    loss += history.history['loss']\n",
    "    val_loss += history.history['val_loss']\n",
    "\n",
    "    scores = (acc, val_acc, loss, val_loss)\n",
    "    #pickles scores\n",
    "    pickle_name = str(t_model)+('_scores.pkl')\n",
    "    pickle.dump(scores, open(pickle_name, 'wb'))\n",
    "\n",
    "    return(model,scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data):\n",
    "    y_pred = model.predict(test_data[0]).argmax(axis=1)\n",
    "    return accuracy_score(test_data[1],y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 28, 28, 3)]       0         \n",
      "                                                                 \n",
      " resizing_3 (Resizing)       (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 1, 1, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,858,890\n",
      "Trainable params: 271,178\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training top layers...\n",
      "157/157 [==============================] - 21s 112ms/step - loss: 1.3227 - accuracy: 0.5658 - val_loss: 0.7913 - val_accuracy: 0.7480\n",
      "Fine tuning all layers...\n",
      "157/157 [==============================] - 244s 1s/step - loss: 0.6976 - accuracy: 0.7678 - val_loss: 0.5449 - val_accuracy: 0.8330\n"
     ]
    }
   ],
   "source": [
    "resnet_model, resnet_score = build_and_train(ResNet50,32,train_data,iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 28, 28, 3)]       0         \n",
      "                                                                 \n",
      " resizing_4 (Resizing)       (None, 75, 75, 3)         0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 1, 1, 2048)        21802784  \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               262272    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,073,962\n",
      "Trainable params: 271,178\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training top layers...\n",
      "157/157 [==============================] - 27s 144ms/step - loss: 0.9651 - accuracy: 0.6812 - val_loss: 0.5777 - val_accuracy: 0.8000\n",
      "Fine tuning all layers...\n",
      "157/157 [==============================] - 143s 828ms/step - loss: 0.4631 - accuracy: 0.8508 - val_loss: 0.4084 - val_accuracy: 0.8640\n"
     ]
    }
   ],
   "source": [
    "inception_model, inception_score = build_and_train(InceptionV3,75,train_data,iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Building model...\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 28, 28, 3)]       0         \n",
      "                                                                 \n",
      " resizing_5 (Resizing)       (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " mobilenet_1.00_224 (Functio  (None, 1, 1, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,368,970\n",
      "Trainable params: 140,106\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training top layers...\n",
      "157/157 [==============================] - 8s 35ms/step - loss: 2.0900 - accuracy: 0.2412 - val_loss: 1.8873 - val_accuracy: 0.3190\n",
      "Fine tuning all layers...\n",
      "157/157 [==============================] - 41s 239ms/step - loss: 1.6098 - accuracy: 0.4346 - val_loss: 1.4487 - val_accuracy: 0.4970\n"
     ]
    }
   ],
   "source": [
    "mobilenet_model, mobilenet_score = build_and_train(MobileNet,32,train_data,iters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39mplot(acc, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mplot(val_acc, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m#plt.ylim([0.8, 1])\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "#plt.ylim([0.8, 1])\n",
    "plt.plot([fc_iter-1,fc_iter-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "#plt.ylim([0, 1.0])\n",
    "plt.plot([fc_iter-1,fc_iter-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "resnet_score = accuracy_score(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
